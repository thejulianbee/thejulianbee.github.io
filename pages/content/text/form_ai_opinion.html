<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>You need to form your own opinion about AI</title>
    <link rel="stylesheet" href="../../../css/form_ai_opinion.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
  <link rel="icon" href="../../../assets/images/julianbee.jpg" type="image/jpg">
</head>
<body>
    <div class="essay-page-container">
        <header class="essay-header">
            <a href="../../../stuff.html" class="back-link">&lt; back to STUFF</a>
            <h1>You need to form your own opinion about AI</h1>
            <p class="post-date">Saturday, May 17th, 2025</p>
            <figure class="hero">
              <img src="../../../assets/images/altman_et_al.jpg" alt="west hall">
            </figure>
        </header>

        <main class="essay-content">
            <article>

                <!-- introduction -->
                <section>
                    <p>This is not pro-AI nor anti-AI. I don't want to tell you how to feel about it, everybody's already doing that. I've been keeping up with the developments and conversation pretty thoroughly since last September, and I can't help but feel that most people who talk about AI are rehashing a second-hand opinion without evaluating it for themselves.</p>
                    <p>And I get it! It's a new, weird technology that people are saying is going to be your boss, your girlfriend, your therapist, etc. It's very easy to listen to an opinion from someone who agrees with you on other issues, check it for face validity, then adopt it if it passes those checks. But the real picture is more confusing and unclear than I think anybody wants to accept. </p>
                    <p>There are many things that are fine to inherit opinions about. I inherit lots of opinions about low-stakes choices without further examination e.g. products to buy, people to interact with. With more information out there than we can process, you need to take shortcuts to be a functional human. But I want to convince you here that AI is not something for which you should take that shortcut. Then I'll tell you about my view of the information landscape and *how* to actually form an opinion about it.</p>
                </section>

                <!-- why do phds anyway -->
                <section>
                    <h2>mental blocks</h2>
                    <p>I need to get to this immediately; a lot of people have mental blocks that keep them from inquiring and critically thinking about AI on their own. I want to flag a few of these and try to ease these splinters out of your brain.</p>
                    <h3>It's scary to think about</h3>
                    <p>No matter what is scary about it, the flight response is inappropriate here. While AI may very well be cause for fear, posts and conversations about AI are not going to jump out of the screen and bite you. This is a good fear to step into; knowing more about it might even increase your fears, but it's better to be prepared.</p>
                    <h3>Interacting with AI is a moral vice</h3>
                    <p>It's a fair position to think it's wrong to interact with AI systems. I don't find a fundamental issue with the position that using AI contributes to a system that's bad for humanity and the environment. But even if people conflate them, there is a major difference between using AI systems and *thinking about* AI systems. Most informations about these systems is freely available on the internet. Many conversations are happening on social media, blog posts, and in-person. The people who make them regularly put out multi-hour discussions on how they're made. You can go through these without paying OpenAI.</p>
                    <h3>It's so awesome!!!</h3>
                    <p>There are a lot of AI boosters who will just mindlessly push it online and in everyday conversation. But just because you use it a lot doesn't mean you're keyed into the important parts of the issue. And if you mostly talk to people who also love AI and chide anybody who strays from total praise, you're not going to have a well-informed opinion on the important issues that come with widespread adop I mentioned the same thing in the previous section: avoid echo chambers. </p>
                    <h3>It's a fad</h3>
                    <p>Even if you think the technology will stall at its current capabilities, everybody is still figuring out how to integrate it into our current systems. There's this idea that (there are no AI shaped holes in the world), and I think that's true. Many companies are trying to just throw a chatbot at you right now, but they are going to keep iterating on these AI-interactions in our day-to-day life for better or worse. We've seen all of the ways that companies can try to quickly integrate AI systems, but over time we'll start to see the methods for integration that took longer to plan and execute. They're being worked on.</p>
                    <h3>I hate tech bros</h3>
                    <p>I largely feel the same. I get irrationally angry whenever I see people post janky ass clips of AI video generations and say that "the movie industry is cooked." I hate the culture that idolizes those who are "cracked." And even though that's the stench of the AI conversation, the technology can be decoupled from those who talk about it.</p>
                    <h3>It's too hard to catch up</h3>
                    <p>Just keep reading. It might feel like this because people like to spew jargon, but you've surely absorbed most of the relevant info already, it just needs to be put together. </p>
                </section>

                <!-- did you like it there though -->
                <section>
                    <h2>basic facts about AI</h2>
                    <p>While I want this piece to be agnostic to most existing opinions about AI, there are some baseline facts that are important to establish. The "AI" I'm referring to here refers to big, black-box models trained on tons of data. Mostly I'm talking about large language models (LLMs: ChatGPT, etc) and diffusion models (the ones that make images and videos), but those two aren't mutually exclusive.</p>
                    <p>The "Some people think" claims are not endorsed, but the fact that they're widely believed among relevant stakeholders is important to understand the the information landscape around the issue.</p>
                </section>

                <!-- so what now -->
                <section>
                    <h2>information minefields</h2>
                    <p>Everybody has an angle. Even people who explicitly try to be unbiased in the way they discuss things. You shouldn't be trying to find the one "right" source of discussion around AI, you *must* seek out many different opinions and perspectives and try to synthesize them into a self-consistent opinion. I want to talk through some of the obvious groups who are talking about it and what I see as their angle.</p>
                    <h3>people in AI</h3>
                    <p>You can probably intuit that people who work in AI tend to be optimistic about AI progress. But since that's so obvious, I think the main issue here is that they tend to greatly discount present and future risks due to AI.</p>
                    <p>One of the immediate present risks of AI that people talk about is that of taking people's jobs. And if you listen to enough podcasts with AI researchers you will hear the same response between different researchers and CEOS: "AI also has the potential to create jobs!"</p>
                    <p>Yes, sure. I buy that. For example, Mark Zuckerberg was recently on a podcast and when asked about this issue gave an example about Meta currently seeing it infeasible to staff call centers, but in a human-supervising-AI situation might now find it profitable. I don't doubt that situations like this exist and that some jobs will get created by AI. But nobody who asks this question earnestly thinks that AI won't create some jobs; it obviously created a lot of AI research jobs. It's the net impact that people are anxious about. </p>
                    <p>In the same interview when asked about whether personalized content and excess time spent on social media could have any negative effects, Zuck brushes it aside and says that he trusts people know their own tastes pretty well.</p>
                    <p>I'm not marking these people as *bad sources* per se, but optimism on capability and discounting of risk is what you have to watch out for when using these CEO statements to synthesize your own views. However, they're also some of the most important people to hear from! Most frontier AI labs are pretty closed-off in terms of sharing the internals of the companies and the models. So anything they say gives us a bit more information about how the leading systems work. Also, as the people in charge of the leading AI organizations it's important to have some model in your head of how these labs will progress over time. </p>
                    <h3>public thinkers</h3>
                    <p>This class of person is kind of vague. Their job title is usually *something* in the intellectual sphere. They make a lot of blog posts, tweets, and salivate at the concept of getting asked to be on a podcast. Luckily, these public thinkers are on all side of the aisle regarding AI capabilities, future capabilities, and the utility of AI. However, this group tends to be overconfident in their claims and also are biased away from simple explanations.</p>
                    <p>Their value as public thinkers is to provide new perspectives, which can be very helpful to the conversation around this technology! But they are incentivized to sound slick rather than be correct. So you get these statements that sound good when confined to 140 characters but don't hold up to so much scrutiny.</p>
                    <h3>NFT-esque shills</h3>
                    <p>These guys annoy me the most, and if you have any sort of bullshit detector than you're probably already avoiding these guys. These are the guys who take some LLM advancement out of context and say that a whole class of jobs is now obsoleted. I think a good portion of these people literally did jump ship from shilling NFTs to making these inflationary posts on Twitter. I hope they're jumping to memecoins.</p>
                    <p>In any case, be wary of anybody making extremely overconfident claims about AI based on a single output that AI produced. These systems are probabilistic and very prompt-sensitive such that no one output can say much. Even so, none of these people ever give a real argument for why the janky Burger King ad they made is destroying the advertising industry.</p>
                    <h3>people in creative work</h3>
                    <p>It's fair that people doing creative work chafed by AI. The fact that these models have been trained on intellectual property without fair accreditation is an issue, and the hunger for data to train the next generation of models is keeping this out of the conversation for the most part. But I think that well-earned frustration gives some creative people a cognitive blind spot when it comes to AI.</p>
                    <p>A common response from an artist to an AI generated image is to say that it sucks for some reason. Often, I agree with these reasons. But the same types of comment were being made a year ago when fingers were a huge issue for image generation. Any specific issue they point out with AI is likely to be solved as we feed more data and compute to the shoggoth. I'm sympathetic to the creative voice here, so I wish more people pointed out the very real issues rather than wasting their energy nitpicking stuff the shills post.</p>
                    <p>So this group's statements typically understate AI's capability.</p>
                </section>
            </article>
        </main>

        <footer class="essay-footer">
            <p>&copy; 2025 The Julian Bee</p>
        </footer>
    </div>
</body>
</html>
